import os
from huggingface_hub import InferenceClient
from dotenv import load_dotenv

load_dotenv()


class DogLLMEngine:
    def __init__(self):
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            raise ValueError("HF_TOKEN is not set in backend/.env")

        # 100% рабочая модель
        self.model_name = "mistralai/Mistral-7B-Instruct-v0.2"

        self.client = InferenceClient(token=hf_token)

    def generate_advice(self, breed: str) -> str:
        prompt = f"""
You are DogExpertGPT, a veterinarian and dog behavior specialist.
Provide a short bullet list about this breed: {breed}.

List:
- Temperament
- Common health risks
- Feeding recommendations
- Activity needs
- Grooming needs
- Training tips
"""

        response = self.client.chat_completion(
            model=self.model_name,
            messages=[
                {"role": "system", "content": "You are a concise dog expert."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=200,
            temperature=0.3,
        )

        return response.choices[0].message["content"]